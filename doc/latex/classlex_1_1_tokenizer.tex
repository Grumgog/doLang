\hypertarget{classlex_1_1_tokenizer}{}\section{Класс lex\+:\+:Tokenizer}
\label{classlex_1_1_tokenizer}\index{lex\+::\+Tokenizer@{lex\+::\+Tokenizer}}


Класс Токенизатора  




{\ttfamily \#include $<$lexer.\+h$>$}

Граф наследования\+:lex\+:\+:Tokenizer\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classlex_1_1_tokenizer}
\end{center}
\end{figure}
\subsection*{Открытые члены}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classlex_1_1_tokenizer_ae074fb2d8d0a4cfc11661c67faad1602}\label{classlex_1_1_tokenizer_ae074fb2d8d0a4cfc11661c67faad1602}} 
virtual std\+::string {\bfseries is\+Token} (const std\+::string str)=0
\end{DoxyCompactItemize}


\subsection{Подробное описание}
Класс Токенизатора 

\begin{DoxyAuthor}{Автор}
Grumgog 
\end{DoxyAuthor}
\begin{DoxyDate}{Дата}
27.\+06.\+2018 
\end{DoxyDate}
\begin{DoxyVersion}{Версия}
0.\+1
\end{DoxyVersion}
Токенизатор -\/ объект для предназначенный для преобразования лексем в токены. 

Объявления и описания членов класса находятся в файле\+:\begin{DoxyCompactItemize}
\item 
C\+:/\+Users/grumgog/\+Desktop/\+Work\+On/do\+Lang/src/lexer.\+h\end{DoxyCompactItemize}
